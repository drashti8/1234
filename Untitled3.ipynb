{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOx872JT2eu8q7IdIst3bbE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drashti8/1234/blob/master/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KUvPu2GcNEJB"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Untitled2.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1xxK8qXqDv95weS-QRXcGe_AJj3wrPEAX\n",
        "\"\"\"\n",
        "\n",
        "import scipy.io\n",
        "import numpy as np\n",
        "\n",
        "class LogisticRegression:\n",
        "    # constructor to take learning rate and number of iterations as parameters\n",
        "    def __init__(self, lr=0.001, epoch=10000):\n",
        "        self.lr = lr\n",
        "        self.epoch = epoch\n",
        "\n",
        "    # data normalization\n",
        "    def normalize(self, X):\n",
        "        X = (X - self.x_mean) / self.x_stddev\n",
        "        return X\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = self.normalize(X)\n",
        "        # matrix of W(i)X(i)\n",
        "        linear = self._linear(X)\n",
        "\n",
        "        # predictions over the sigmoid function\n",
        "        preds = self._non_linear(linear)\n",
        "\n",
        "        # returning 1 or 0\n",
        "        return (preds >= 0.5).astype('int')\n",
        "\n",
        "     # sigmoid function, calculating the non-linear score\n",
        "    def _non_linear(self, X):\n",
        "        return 1 / (1 + np.exp(-X))\n",
        "\n",
        "     # calculating the linear score\n",
        "    def _linear(self, X):\n",
        "        return np.dot(X, self.weights) + self.bias\n",
        "\n",
        "\n",
        "    def initialize_weights(self, X):\n",
        "        # setting weight as a 2*1 matrix with random values\n",
        "        self.weights = np.random.rand(X.shape[1], 1)\n",
        "        # setting bias = 0 for all the features initially for prediction\n",
        "        self.bias = np.zeros((1,))\n",
        "\n",
        "    def fit(self, X_train, Y_train):\n",
        "\n",
        "        self.initialize_weights(X_train)\n",
        "\n",
        "        # get mean and stddev for normalization\n",
        "        self.x_mean = X_train.mean(axis=0).T\n",
        "        self.x_stddev = X_train.std(axis=0).T\n",
        "\n",
        "        # normalize data\n",
        "        X_train = self.normalize(X_train)\n",
        "\n",
        "        # Run gradient descent for n iterations\n",
        "        for i in range(self.epoch):\n",
        "            # make normalized predictions\n",
        "            probs = self._non_linear(self._linear(X_train))\n",
        "            diff = probs - Y_train\n",
        "\n",
        "            # calculating the gradient\n",
        "            delta_w = np.matmul(np.transpose(X_train),diff)\n",
        "            delta_b = np.mean(diff)\n",
        "\n",
        "            # update weights\n",
        "            self.weights = self.weights - np.dot(self.lr,delta_w)\n",
        "            self.bias = self.bias - np.dot(self.lr,delta_b)\n",
        "        return self\n",
        "\n",
        "    # calculating accuracy\n",
        "    def accuracy(self, X, y):\n",
        "        preds = self.predict(X)\n",
        "        return np.mean(preds == y)\n",
        "\n",
        "    #calculating cross entropy\n",
        "    def loss(self, X, y):\n",
        "        probs = self._non_linear(self._linear(X))\n",
        "\n",
        "        # entropy when true class is positive\n",
        "        pos_log = y * np.log(probs + 1e-15)\n",
        "        # entropy when true class is negative\n",
        "        neg_log = (1 - y) * np.log((1 - probs) + 1e-15)\n",
        "        #mean of entropy\n",
        "        l = -np.mean(pos_log + neg_log)\n",
        "        return l\n",
        "\n",
        "\n",
        "def data_Extraction():\n",
        "\n",
        "    Numpyfile= scipy.io.loadmat('/content/mnist_data.mat')\n",
        "    trX = Numpyfile['trX']\n",
        "    trY = Numpyfile['trY']\n",
        "    tsX = Numpyfile['tsX']\n",
        "    tsY = Numpyfile['tsY']\n",
        "    return trX,trY,tsX,tsY\n",
        "\n",
        "\n",
        "def feature_Extraction(X):\n",
        "    dataset = []\n",
        "    for i in range(len(X)):\n",
        "        temp = np.array([np.mean(X[i]), np.std(X[i])])\n",
        "        dataset.append(temp)\n",
        "    dataset = np.array(dataset, dtype=np.float128)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "#if __name__ == \"__main__\":\n",
        "def main():\n",
        "    print(\"Logistic Regression\")\n",
        "    #data Extraction\n",
        "    trX,trY,tsX,tsY = data_Extraction()\n",
        "\n",
        "    #data transformation\n",
        "    trY = np.transpose(trY)\n",
        "    tsY = np.transpose(tsY)\n",
        "\n",
        "\n",
        "    #feature extraction\n",
        "\n",
        "    #training dataset\n",
        "    dataset = feature_Extraction(trX)\n",
        "\n",
        "    #testing dataset\n",
        "    datatest = feature_Extraction(tsX)\n",
        "\n",
        "    lr = LogisticRegression()\n",
        "    lr.fit(dataset, trY)\n",
        "    print()\n",
        "    print('Loss on Testing Data: {:.2f}'.format(lr.loss(datatest, tsY)))\n",
        "    print('Overall Accuracy: {:.2f}%'.format(lr.accuracy(datatest, tsY) * 100))\n",
        "    print('*********************************')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Untitled1.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1ijlUX2s3j3buzRfqtiwD0K_Zi5a9Xhvl\n",
        "\"\"\"\n",
        "\n",
        "import scipy.io\n",
        "import numpy as np\n",
        "\n",
        "def naive_bayes(trX,trY, tsX, tsY):\n",
        "\n",
        "    #calculating Priors\n",
        "    rows, cols  = np.shape(trX)\n",
        "    count_7 = len(trY[trY == 0.0])\n",
        "    count_8 = len(trY[trY == 1.0])\n",
        "\n",
        "    P_7 = count_7/rows\n",
        "    P_8 = count_8/rows\n",
        "\n",
        "    print('Let w1 denote class of digit 7 and w2 denote class of digit 8')\n",
        "\n",
        "    print('Prior(w1): {:.2f}%'.format(P_7))\n",
        "    print('Prior(w2): {:.2f}%'.format(P_8))\n",
        "\n",
        "\n",
        "    #data seperation\n",
        "    dataset_7 = []\n",
        "    dataset_8 = []\n",
        "    for i in range(len(trY[0])):\n",
        "        if trY[0][i] == 0.0:\n",
        "            dataset_7.append(trX[i])\n",
        "        else:\n",
        "            dataset_8.append(trX[i])\n",
        "\n",
        "    #feature Extraction\n",
        "    features_7 = []\n",
        "    for i in range(count_7):\n",
        "        temp = np.array([np.mean(dataset_7[i]), np.std(dataset_7[i])])\n",
        "        temp.shape = (2,1)\n",
        "        features_7.append(temp)\n",
        "    features_8 = []\n",
        "    for i in range(count_8):\n",
        "        temp = np.array([np.mean(dataset_8[i]), np.std(dataset_8[i])])\n",
        "        temp.shape = (2,1)\n",
        "        features_8.append(temp)\n",
        "\n",
        "    mean_feature17 = np.mean([i[0][0] for i in features_7])\n",
        "    mean_feature27 = np.mean([i[1][0] for i in features_7])\n",
        "\n",
        "    var_feature17 = np.var([i[0][0] for i in features_7])\n",
        "    var_feature27 = np.var([i[1][0] for i in features_7])\n",
        "\n",
        "    mean_feature18 = np.mean([i[0][0] for i in features_8])\n",
        "    mean_feature28 = np.mean([i[1][0] for i in features_8])\n",
        "\n",
        "    var_feature18 = np.var([i[0][0] for i in features_8])\n",
        "    var_feature28 = np.var([i[1][0] for i in features_8])\n",
        "\n",
        "    mean_7 = np.array([mean_feature17, mean_feature27])\n",
        "    mean_8 = np.array([mean_feature18, mean_feature28])\n",
        "\n",
        "    #data transformation\n",
        "    mean_7.shape = (2,1)\n",
        "    mean_8.shape = (2,1)\n",
        "\n",
        "    #coveriance matrix\n",
        "    cov_7 = np.array([[var_feature17,0], [0,var_feature27]])\n",
        "    cov_8 = np.array([[var_feature18,0], [0,var_feature28]])\n",
        "\n",
        "    #multivariate normal probablity density function\n",
        "    def multivariate_normal(x, d, mean, covariance):\n",
        "        x_m = x - mean\n",
        "        return (1. / (np.sqrt((2 * np.pi)**d * np.linalg.det(covariance))) *\n",
        "                    np.exp(-(np.linalg.solve(covariance, x_m).T.dot(x_m)) / 2))\n",
        "\n",
        "    # calculating accuracy\n",
        "    correct7 = 0\n",
        "    correct8 = 0\n",
        "    total7 = list(tsY[0]).count(0.0)\n",
        "    total8 = list(tsY[0]).count(1.0)\n",
        "\n",
        "    for i in range(len(tsY[0])):\n",
        "        temp = np.array([np.mean(tsX[i]), np.std(tsX[i])])\n",
        "        temp.shape = (2,1)\n",
        "        #calculating Probablity of predicted nature of digit\n",
        "        p7 = multivariate_normal(temp,2,mean_7,cov_7)*P_7\n",
        "        p8 = multivariate_normal(temp,2,mean_8,cov_8)*P_8\n",
        "        if (p7>p8 and tsY[0][i] == 0.0):\n",
        "          correct7+=1\n",
        "        if (p7<p8 and tsY[0][i] == 1.0):\n",
        "          correct8 +=1\n",
        "\n",
        "\n",
        "    print()\n",
        "    print('Accuracy on Testing Data for \"7\": {:.2f}%'.format(100*correct7/total7))\n",
        "    print('Accuracy on Testing Data for \"8\": {:.2f}%'.format(100*correct8/total8))\n",
        "    print('Overall Accuracy: {:.2f}%'.format((correct7+correct8)*100/(total7+total8)))\n",
        "    print('********************************')\n",
        "\n",
        "#if __name__ == \"__main__\":\n",
        "def main():\n",
        "    print('Naive Bayes Classifier')\n",
        "    # data Extraction\n",
        "    Numpyfile= scipy.io.loadmat('/content/drive/MyDrive/CSE575-Project1-Data/train_0_img.mat')\n",
        "    trX = Numpyfile['trX']\n",
        "    trY = Numpyfile['trY']\n",
        "    tsX = Numpyfile['tsX']\n",
        "    tsY = Numpyfile['tsY']\n",
        "\n",
        "    naive_bayes(trX, trY, tsX, tsY)\n"
      ],
      "metadata": {
        "id": "uqOueTgVOGuo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.io\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "XDfqLTy-Ndxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMjGvnMUONtf",
        "outputId": "59639e12-bf93-46ae-f64e-15056d685987"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_bayes(trX,trY, tsX, tsY):\n",
        "\n",
        "    #calculating Priors\n",
        "    rows, cols  = np.shape(trX)\n",
        "    count_7 = len(trY[trY == 0.0])\n",
        "    count_8 = len(trY[trY == 1.0])\n",
        "\n",
        "    P_7 = count_7/rows\n",
        "    P_8 = count_8/rows\n",
        "\n",
        "    print('Let w1 denote class of digit 7 and w2 denote class of digit 8')\n",
        "\n",
        "    print('Prior(w1): {:.2f}%'.format(P_7))\n",
        "    print('Prior(w2): {:.2f}%'.format(P_8))\n"
      ],
      "metadata": {
        "id": "A4mbH-toNkBK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zJr-co-YNnsK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}